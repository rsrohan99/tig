from llama_index.core.agent.workflow import AgentOutput, ToolCallResult, ToolCall
from llama_index.core.llms import ChatMessage


async def log_events(handler, chat_history=None):
    """
    Log events from the handler.
    """
    current_agent = None
    async for event in handler.stream_events():
        if (
            hasattr(event, "current_agent_name")
            and event.current_agent_name != current_agent
        ):
            current_agent = event.current_agent_name
            print(f"\n{'=' * 50}")
            print(f"ğŸ¤– Agent: {current_agent}")
            print(f"{'=' * 50}\n")
        elif isinstance(event, AgentOutput):
            if event.response.content:
                if chat_history is not None:
                    chat_history.append(
                        ChatMessage(
                            role="assistant",
                            content=event.response.content,
                        )
                    )
                print("ğŸ“¤ Output:", event.response.content)
            if event.tool_calls:
                print(
                    "ğŸ› ï¸  Planning to use tools:",
                    [call.tool_name for call in event.tool_calls],
                )
        elif isinstance(event, ToolCallResult):
            print(f"ğŸ”§ Tool Result ({event.tool_name}):")
            # print(f"  Arguments: {event.tool_kwargs}")
            print(f"  Output: {event.tool_output}")
        elif isinstance(event, ToolCall):
            print(f"ğŸ”¨ Calling Tool: {event.tool_name}")
            print(f"  With arguments: {event.tool_kwargs}")
